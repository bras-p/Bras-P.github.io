[{"authors":null,"categories":null,"content":"Pierre Bras is a third-year PhD student at LPSM Sorbonne Université under the direction of Gilles Pagès. His research focuses on Machine Learning, numerical methods for probability and statistics and on applications to Finance.\n","date":1694304e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1694304e3,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Pierre Bras is a third-year PhD student at LPSM Sorbonne Université under the direction of Gilles Pagès. His research focuses on Machine Learning, numerical methods for probability and statistics and on applications to Finance.","tags":null,"title":"Pierre Bras","type":"authors"},{"authors":[],"categories":null,"content":"Slides\n","date":1702034100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702034100,"objectID":"93670e52ab63a177174bc53c8bae2a73","permalink":"https://Bras-P.github.io/talk/seminaire-de-mathematiques-appliquees-du-college-de-france/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/talk/seminaire-de-mathematiques-appliquees-du-college-de-france/","section":"event","summary":"Algorithmes adaptatifs de gradient-Langevin pour l'optimisation stochastique et l'inférence Bayésienne","tags":[],"title":"Séminaire de Mathématiques Appliquées du Collège de France","type":"event"},{"authors":["Pierre Bras"],"categories":null,"content":"\r","date":1694304e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694304e3,"objectID":"6aad632eb6eb4d81dc799d4ff1bf17a6","permalink":"https://Bras-P.github.io/publication/phd/","publishdate":"2023-09-10T00:00:00Z","relpermalink":"/publication/phd/","section":"publication","summary":"This thesis focuses on adaptive Stochastic Gradient Langevin Dynamics (SGLD) algorithms to solve optimization and Bayesian inference problems. SGLD algorithms consist in a stochastic gradient descent with exogenous noise added in order to escape local minima and saddle points. Contrary to the classic Langevin Stochastic Differential Equation, we study the case where the exogenous noise is adaptive i.e. not constant but depends on the position of the procedure. In a first part we prove the convergence of SGLD algorithms for the L1-Wasserstein distance and for the Total Variation distance. In a second part we apply SGLD algorithms to optimization and inference problems arising in Machine Learning and in Numerical Probability and we introduce the Layer Langevin algorithm. A last part is devoted to the numerical simulation of stochastic processes.","tags":["Stochastic Optimization","Gradient Descent","Langevin Dynamics","Simulated Annealing","Machine Learning","Deep Learning","Neural Networks","Euler-Maruyama Scheme","Bayesian Inference","Stochastic Control","Monte Carlo","MCMC"],"title":"Adaptive Gradient Langevin Algorithms for Stochastic Optimization and Bayesian Inference","type":"publication"},{"authors":["Pierre Bras","Gilles Pagès"],"categories":null,"content":"\r","date":1690156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690156800,"objectID":"a98eba3d8473ea2c32d93e4794036d61","permalink":"https://Bras-P.github.io/publication/correlation_optimal/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/publication/correlation_optimal/","section":"publication","summary":"We propose a new algorithm for variance reduction when estimating f(XT) where X is the solution to some stochastic differential equation and f is a test function. The new estimator is (f(X1T)+f(X2T))/2, where X1 and X2 have same marginal law as X but are pathwise correlated so that to reduce the variance. The optimal correlation function ρ is approximated by a deep neural network and is calibrated along the trajectories of (X1,X2) by policy gradient and reinforcement learning techniques. Finding an optimal coupling given marginal laws has links with maximum optimal transport.","tags":["Reinforcement Learning","Policy Gradient","Stochastic Differential Equation","Variance Reduction","Monte Carlo","Stochastic Control","Optimal Transport"],"title":"Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport","type":"publication"},{"authors":null,"categories":null,"content":"","date":1689811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689811200,"objectID":"20006664f267bf96ca727b65e370f840","permalink":"https://Bras-P.github.io/project/relocor/","publishdate":"2023-07-20T00:00:00Z","relpermalink":"/project/relocor/","section":"project","summary":"Implementation of the package relocor: REinforcement Learning Optimal CORrelation search, a stochastic control and reinforcement-learning based method for variance reduction in Monte Carlo simulation of stochastic differential equations.","tags":null,"title":"relocor: REinforcement Learning Optimal CORrelation search","type":"project"},{"authors":[],"categories":null,"content":"Slides\n","date":1687953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687953600,"objectID":"6b4d1c3c7b988100d7440e9d6592383b","permalink":"https://Bras-P.github.io/talk/14th-international-conference-on-monte-carlo-methods-and-applications/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/talk/14th-international-conference-on-monte-carlo-methods-and-applications/","section":"event","summary":"Convergence of Langevin-Simulated Annealing algorithms with multiplicative noise","tags":[],"title":"14th International Conference on Monte Carlo Methods and Applications","type":"event"},{"authors":[],"categories":null,"content":"\r","date":1687539600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687539600,"objectID":"fabf3145e1723f3e2dc72d115fdef212","permalink":"https://Bras-P.github.io/talk/international-neural-network-society-workshop-on-deep-learning-innovations-and-applications-inns-dlia-2023/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/international-neural-network-society-workshop-on-deep-learning-innovations-and-applications-inns-dlia-2023/","section":"event","summary":"Langevin Algorithms for Very Deep Neural Networks with application to Image Classification","tags":[],"title":"International Neural Network Society Workshop on Deep Learning Innovations and Applications INNS DLIA 2023","type":"event"},{"authors":[],"categories":null,"content":"\r","date":1687107600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687107600,"objectID":"83a12de5e52118352dd199ab9b6bbf4e","permalink":"https://Bras-P.github.io/talk/international-joint-conference-on-neural-networks-ijcnn-2023/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/international-joint-conference-on-neural-networks-ijcnn-2023/","section":"event","summary":"Langevin Algorithms for Markovian Neural Networks and Deep Stochastic Control","tags":[],"title":"International Joint Conference on Neural Networks IJCNN 2023","type":"event"},{"authors":null,"categories":null,"content":"","date":1671494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671494400,"objectID":"14b29de690408994a78e932f56f86874","permalink":"https://Bras-P.github.io/project/markovian_nn/","publishdate":"2022-12-20T00:00:00Z","relpermalink":"/project/markovian_nn/","section":"project","summary":"In this repository we give the implementation of Langevin and Layer Langevin optimizers as instances of the TensorFlow tf.keras.optimizers.Optimizer base class and we compare Langevin and non-Langevin optimizers for the training of various stochastic control problems.","tags":null,"title":"Langevin algorithms for Markovian Neural Networks and Deep Stochastic control","type":"project"},{"authors":["Pierre Bras","Gilles Pagès"],"categories":null,"content":"\r","date":1671494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671494400,"objectID":"73d514747c5314436f0704971da67cec","permalink":"https://Bras-P.github.io/publication/markovian_nn/","publishdate":"2022-12-20T00:00:00Z","relpermalink":"/publication/markovian_nn/","section":"publication","summary":"Stochastic Gradient Descent Langevin Dynamics (SGLD) algorithms, which add noise to the classic gradient descent, are known to improve the training of neural networks in some cases where the neural network is very deep. In this paper we study the possibilities of training acceleration for the numerical resolution of stochastic control problems through gradient descent, where the control is parametrized by a neural network. If the control is applied at many discretization times then solving the stochastic control problem reduces to minimizing the loss of a very deep neural network. We numerically show that Langevin algorithms improve the training on various stochastic control problems like hedging and resource management, and for different choices of gradient descent methods.","tags":["Langevin algorithm","SGLD","Markovian neural network","Stochastic control","Deep neural network","Stochastic optimization"],"title":"Langevin algorithms for Markovian Neural Networks and Deep Stochastic control","type":"publication"},{"authors":["Pierre Bras"],"categories":null,"content":"\r","date":1671408e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671408e3,"objectID":"b6f2d1f8f9529609f94141f4bc2f5651","permalink":"https://Bras-P.github.io/publication/empiric_minimizer/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/publication/empiric_minimizer/","section":"publication","summary":"For V:Rd→R coercive, we study the convergence rate for the L1-distance of the empiric minimizer, which is the true minimum of the function V sampled with noise with a finite number n of samples, to the minimum of V. We show that in general, for unbounded functions with fast growth, the convergence rate is bounded above by an n−1/q, where q is the dimension of the latent random variable and where an=o(nε) for every ε\u003e0. We then present applications to optimization problems arising in Machine Learning and in Monte Carlo simulation.","tags":["Stochastic Optimization","Empiric measure","Empiric minimizer"],"title":"A note on $L^1$-Convergence of the Empiric Minimizer for unbounded functions with fast growth","type":"publication"},{"authors":null,"categories":null,"content":"","date":1671408e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671408e3,"objectID":"70f8caa266139191993e679c4a1fd5eb","permalink":"https://Bras-P.github.io/project/deep_nn/","publishdate":"2022-12-19T00:00:00Z","relpermalink":"/project/deep_nn/","section":"project","summary":"In this repository we give the implementation of Langevin and Layer Langevin optimizers as instances of the TensorFlow tf.keras.optimizers.Optimizer base class and we compare Langevin and non-Langevin optimizers for the training of various problems for image classification.","tags":null,"title":"Langevin algorithms for very deep Neural Networks with applications to image classification","type":"project"},{"authors":["Pierre Bras"],"categories":null,"content":"\r","date":1671408e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671408e3,"objectID":"1640e081828846d5eae18ef3a66e1d2b","permalink":"https://Bras-P.github.io/publication/deep_nn/","publishdate":"2022-12-19T00:00:00Z","relpermalink":"/publication/deep_nn/","section":"publication","summary":"Training a very deep neural network is a challenging task, as the deeper a neural network is, the more non-linear it is. We compare the performances of various preconditioned Langevin algorithms with their non-Langevin counterparts for the training of neural networks of increasing depth. For shallow neural networks, Langevin algorithms do not lead to any improvement, however the deeper the network is and the greater are the gains provided by Langevin algorithms. Adding noise to the gradient descent allows to escape from local traps, which are more frequent for very deep neural networks. Since the deepest layers of a network are the most non-linear ones, we introduce a new Langevin algorithm called Layer Langevin, which consists in adding Langevin noise only to the weights associated to the deepest layers. We then prove the benefits of Langevin and Layer Langevin algorithms for the training of popular deep residual architectures for image classification.","tags":["Langevin algorithm","SGLD","Deep neural network","Image classification","ResNet"],"title":"Langevin algorithms for very deep Neural Networks with applications to image classification","type":"publication"},{"authors":[],"categories":null,"content":"Slides\n","date":1670259600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670259600,"objectID":"ebe3bf44e8d1590a098ea5762291e3fd","permalink":"https://Bras-P.github.io/talk/cermics-groupe-de-travail-methodes-stochastiques-et-finance/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/cermics-groupe-de-travail-methodes-stochastiques-et-finance/","section":"event","summary":"Convergence of Langevin-Simulated Annealing algorithms with multiplicative noise","tags":[],"title":"CERMICS - Groupe de Travail Méthodes Stochastiques et Finance","type":"event"},{"authors":[],"categories":null,"content":"Slides\nReference article\n","date":1653325200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653325200,"objectID":"fc1ad3ef08fb0dedd333fe950c3fe33f","permalink":"https://Bras-P.github.io/talk/groupe-de-travail-des-thesards-du-lpsm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/groupe-de-travail-des-thesards-du-lpsm/","section":"event","summary":"Stochastic Gradient Descent and Langevin-Simulated Annealing algorithms","tags":[],"title":"Groupe de travail des thésards du LPSM","type":"event"},{"authors":null,"categories":null,"content":"","date":1643932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643932800,"objectID":"80c095a1065268982beafe903b0f75c6","permalink":"https://Bras-P.github.io/project/singular-hessian/","publishdate":"2022-02-04T00:00:00Z","relpermalink":"/project/singular-hessian/","section":"project","summary":"This repository contains a notebook showing how we can compute the higher order nested expansion of some objective function to be minimized in problems where the Hessian matrix at the minimum is singular. In this example, we train a neural network and study the behaviour of the loss function in the neighbourhood of the (empirical) minimum. The machine learning library that is used is TensorFlow.","tags":null,"title":"Convergence rates of Gibbs measures with degenerate minimum","type":"project"},{"authors":null,"categories":null,"content":"","date":1642723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642723200,"objectID":"aac26bc2bfb5c4b792f298bfb450f68c","permalink":"https://Bras-P.github.io/project/langevin-sa/","publishdate":"2022-01-21T00:00:00Z","relpermalink":"/project/langevin-sa/","section":"project","summary":"This repository contains the source code for the implementation of various Langevin optimizers in TensorFlow and a demonstration notebook.","tags":null,"title":"Convergence of Langevin-Simulated Annealing algorithms with multiplicative noise","type":"project"},{"authors":null,"categories":null,"content":"","date":1641254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641254400,"objectID":"4e0d81326ce81f78b91388a26a362648","permalink":"https://Bras-P.github.io/project/reflected-brownian-motion/","publishdate":"2022-01-04T00:00:00Z","relpermalink":"/project/reflected-brownian-motion/","section":"project","summary":"This is the Python code for the algorithms that are described in the paper Simulation of stopped and reflected Brownian motion, simulation of stopped and reflected processes. The notebook Simulations.ipynb shows how to use the source code rmb.py.","tags":null,"title":"Simulation of Reflected Brownian motion on two dimensional wedges","type":"project"},{"authors":[],"categories":null,"content":"Slides\n","date":1639008e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639008e3,"objectID":"be0f62b7b987c4d2114999ba726337fd","permalink":"https://Bras-P.github.io/talk/osaka-webinar-on-mathematical-finance/","publishdate":"2021-12-09T00:00:00Z","relpermalink":"/talk/osaka-webinar-on-mathematical-finance/","section":"event","summary":"Convergence of Langevin-Simulated Annealing algorithms with multiplicative noise","tags":[],"title":"Osaka Webinar on Mathematical Finance","type":"event"},{"authors":["Pierre Bras","Gilles Pagès","Fabien Panloup"],"categories":null,"content":"\rGilles Pagès’ website\nFabien Panloup’s website\n","date":1637193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637193600,"objectID":"076ab0d6ff41cc0de4eebc3f742a7ab3","permalink":"https://Bras-P.github.io/publication/dtv-small-time/","publishdate":"2021-10-08T00:00:00Z","relpermalink":"/publication/dtv-small-time/","section":"publication","summary":"We give bounds for the total variation distance between the solutions to two stochastic differential equations starting at the same point and with close coefficients, which applies in particular to the distance between an exact solution and its Euler-Maruyama scheme in small time. We show that for small t, the total variation distance is of order t r/(2r+1) if the noise coefficient σ of the SDE is elliptic and C 2r b*, r ∈ N and if the drift is C1 with bounded derivatives, using multi-step Richardson-Romberg extrapolation. We do not require the drift to be bounded. Then we prove with a counterexample that we cannot achieve a bound better than t 1/2 in general.","tags":["Stochastic differential equation","Euler-Maruyama scheme","Total variation"],"title":"Total variation distance between two diffusions in small time with unbounded drift: application to the Euler-Maruyama scheme","type":"publication"},{"authors":null,"categories":null,"content":"Laboratory / Établissement LPSM Laboratoire de Probabilités, Statistiques et Modélisation Sorbonne Université 4 Place Jussieu 75005 Paris France\nPublication director / Directeur de publication Pierre Bras, PhD Student\nInformations de contact / Contact informations\nHost / Hébergeur LPSM\n","date":1635980400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635980400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"https://Bras-P.github.io/terms/","publishdate":"2021-11-04T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"Laboratory / Établissement LPSM Laboratoire de Probabilités, Statistiques et Modélisation Sorbonne Université 4 Place Jussieu 75005 Paris France\nPublication director / Directeur de publication Pierre Bras, PhD Student\nInformations de contact / Contact informations","tags":null,"title":"Legal notice / Mentions légales","type":"page"},{"authors":[],"categories":null,"content":"Slides\n","date":1634638500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634638500,"objectID":"102dcf616c48bcb952d289224be33234","permalink":"https://Bras-P.github.io/talk/introduction-to-entropic-optimal-transport-phd-seminar/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/introduction-to-entropic-optimal-transport-phd-seminar/","section":"event","summary":"Convergence of Langevin-Simulated Annealing algorithms with multiplicative noise in L1-Wasserstein distance","tags":[],"title":"Introduction to Entropic Optimal Transport - PhD Seminar","type":"event"},{"authors":["Pierre Bras","Gilles Pagès"],"categories":null,"content":"\rGilles Pagès’ website\n","date":1632355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632355200,"objectID":"8ba4d88ce751ec078330b721c7e28f7d","permalink":"https://Bras-P.github.io/publication/langevin/","publishdate":"2021-10-08T00:00:00Z","relpermalink":"/publication/langevin/","section":"publication","summary":"We study the convergence of Langevin-Simulated Annealing type algorithms with multiplicative noise, i.e. for V:Rd→R a potential function to minimize, we consider the stochastic equation dYt=−σσ⊤∇V(Yt)dt+a(t)σ(Yt)dWt+a(t)2Υ(Yt)dt, where (Wt) is a Brownian motion, where σ:Rd→Md(R) is an adaptive (multiplicative) noise, where a:R+→R+ is a function decreasing to 0 and where Υ is a correction term. This setting can be applied to optimization problems arising in Machine Learning. The case where σ is a constant matrix has been extensively studied however little attention has been paid to the general case. We prove the convergence for the L1-Wasserstein distance of Yt and of the associated Euler-scheme Y¯t to some measure ν⋆ which is supported by argmin(V) and give rates of convergence to the instantaneous Gibbs measure νa(t) of density ∝exp(−2V(x)/a(t)2). To do so, we first consider the case where a is a piecewise constant function. We find again the classical schedule a(t)=Alog−1/2(t). We then prove the convergence for the general case by giving bounds for the Wasserstein distance to the stepwise constant case using ergodicity properties.","tags":["Stochastic optimization","Langevin equation","Simulated annealing","Neural networks"],"title":"Convergence of Langevin-Simulated Annealing algorithms with multiplicative noise ","type":"publication"},{"authors":["Pierre Bras","Gilles Pagès"],"categories":null,"content":"\rGilles Pagès’ website\n","date":1632355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632355200,"objectID":"0fc939240b6df0868473b89a0eb67755","permalink":"https://Bras-P.github.io/publication/langevin_dtv/","publishdate":"2021-10-08T00:00:00Z","relpermalink":"/publication/langevin_dtv/","section":"publication","summary":"We study the convergence of Langevin-Simulated Annealing type algorithms with multiplicative noise, i.e. for V:Rd→R a potential function to minimize, we consider the stochastic differential equation dYt=−σσ⊤∇V(Yt)dt+a(t)σ(Yt)dWt+a(t)2Υ(Yt)dt, where (Wt) is a Brownian motion, where σ:Rd→Md(R) is an adaptive (multiplicative) noise, where a:R+→R+ is a function decreasing to 0 and where Υ is a correction term. Allowing σ to depend on the position brings faster convergence in comparison with the classical Langevin equation dYt=−∇V(Yt)dt+σdWt. In a previous paper we established the convergence in L1-Wasserstein distance of Yt and of its associated Euler scheme Y¯t to argmin(V) with the classical schedule a(t)=Alog−1/2(t). In the present paper we prove the convergence in total variation distance. The total variation case appears more demanding to deal with and requires regularization lemmas.","tags":["Stochastic optimization","Langevin equation","Simulated annealing","Neural networks"],"title":"Convergence of Langevin-Simulated Annealing algorithms with multiplicative noise II: Total Variation","type":"publication"},{"authors":[],"categories":null,"content":"Slides\n","date":1623317400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623317400,"objectID":"bffacf521554256061e977898be343db","permalink":"https://Bras-P.github.io/talk/ritsumeikan-university-math-fi-seminar/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/ritsumeikan-university-math-fi-seminar/","section":"event","summary":"Convergence rates of Gibbs measures with degenerate minimum","tags":[],"title":"Ritsumeikan University Math-Fi Seminar","type":"event"},{"authors":["Pierre Bras","Arturo Kohatsu-Higa"],"categories":null,"content":"\rArturo Kohatsu-Higa’s website\n","date":1622851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622851200,"objectID":"e8847fc06d417ae018ef8aab971bff5a","permalink":"https://Bras-P.github.io/publication/simulation-wedge/","publishdate":"2021-10-08T00:00:00Z","relpermalink":"/publication/simulation-wedge/","section":"publication","summary":"We study a correlated Brownian motion in two dimensions, which is reflected, stopped or killed in a wedge represented as the intersection of two half spaces. First, we provide explicit density formulas, hinted by the method of images. These explicit expressions rely on infinite oscillating sums of Bessel functions and may demand computationally costly procedures. We propose suitable recursive algorithms for the simulation of the laws of reflected and stopped Brownian motion which are based on generalizations of the reflection principle in two dimensions. We study and give bounds for the complexity of the proposed algorithms.","tags":["Simulation","Monte Carlo","Reflected Brownian motion"],"title":"Simulation of Reflected Brownian motion on two dimensional wedges","type":"publication"},{"authors":[],"categories":null,"content":"Slides\n","date":1621526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621526400,"objectID":"39c23021344b981d84b371c434f54c81","permalink":"https://Bras-P.github.io/talk/seminaire-doctorants/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/seminaire-doctorants/","section":"event","summary":"Convergence rates of Gibbs measures with degenerate minimum","tags":[],"title":"Séminaire doctorants","type":"event"},{"authors":["Pierre Bras"],"categories":null,"content":"\r","date":1612310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612310400,"objectID":"c1dcf42f20519e8bf9f3779ba7e239ca","permalink":"https://Bras-P.github.io/publication/hilbert/","publishdate":"2021-10-08T00:00:00Z","relpermalink":"/publication/hilbert/","section":"publication","summary":"We study convergence rates for Gibbs measures, with density proportional to e−f(x)/t, as t→0 where f:Rd→R admits a unique global minimum at x⋆. We focus on the case where the Hessian is not definite at x⋆. We assume instead that the minimum is strictly polynomial and give a higher order nested expansion of f at x⋆, which depends on every coordinate. We give an algorithm yielding such a decomposition if the polynomial order of x⋆ is no more than 8, in connection with Hilbert's 17th problem. However, we prove that the case where the order is 10 or higher is fundamentally different and that further assumptions are needed. We then give the rate of convergence of Gibbs measures using this expansion. Finally we adapt our results to the multiple well case.","tags":["Gibbs measures","Stochastic optimization"],"title":"Convergence rates of Gibbs measures with degenerate minimum","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34;\rif porridge == \u0026#34;blueberry\u0026#34;:\rprint(\u0026#34;Eating...\u0026#34;)\rMath In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\rPress the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}}\r{{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}\rCustom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\rQuestions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://Bras-P.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]